<?xml version="1.0"?>
<TeamMentor_Article Metadata_Hash="0" Content_Hash="0">
  <Metadata>
    <Id>ab0d66cf-2ae9-4869-83b8-a867ca9cdc90</Id>
    <Id_History>6f15d4c2-1a0c-4ef5-a50c-44c639bd898d,</Id_History>
    <Library_Id>c037d0d2-0617-44f3-b846-21dc3d02c4f8</Library_Id>
    <Title>Block Search Engines from Sensitive Areas</Title>
    <Category>Hardening</Category>
    <Phase>Deployment</Phase>
    <Technology>Web Application</Technology>
    <Type>Guideline</Type>
    <DirectLink>Block Search Engines from Sensitive Areas</DirectLink>
    <Author />
    <Priority />
    <Status />
  </Metadata>
  <Content Sanitized="false" DataType="wikitext">
    <Data><![CDATA[==Applies To==
* Web Applications

==What to Do==
Use a robots.txt file to block search engines from indexing sensitive data on your site.

==Why==
If a search engine indexes sensitive data on your web site, an attacker will be able to retrieve that data from the search engine.

==How==
To block search engines from indexing sensitive data on your site by using a robots.txt file:

# **Identify sensitive URI locations.** Enumerate all the URIs that lead to sensitive data in your application. These URIs will usually correspond to the filesystem structure of the application, but sometimes they will be produced dynamically by the application itself. Whatever the case, consider how the URIs look from the perspective of the client when enumerating the locations of the sensitive data. It is strongly recommended to prevent indexing of any administrative pages.
# **Write a robots.txt file.** The robots.txt file has a simple syntax. Study the reference at the link at the bottom of this page, and write a robots.txt file that prevents indexing of all the sensitive URI locations.
# **Upload the robots.txt file to the web site.** Upload the robots.txt file to the web root directory on your application server. 
# **(Optional) Examine search engine caches.** Use popular web search engines to execute search queries that include sensitive URI locations on your server. See if these locations have already been cached. If sensitive data has already been cached, it should be purged automatically some time later when the search engine crawls your site again and parses the robots.txt file correctly. If that is not acceptable, consider contacting the search engine operator and asking them to remove the sensitive data from their caches.

==Additional Resources==
* For information about the robots.txt file syntax, see http://www.w3.org/TR/html4/appendix/notes.html#h-B.4.1.1

]]></Data>
  </Content>
</TeamMentor_Article>